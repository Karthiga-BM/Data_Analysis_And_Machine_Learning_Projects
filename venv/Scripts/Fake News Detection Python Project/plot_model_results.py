
#
from matplotlib import pyplot as plt
# #Accuracy Comparison for all four models with 5 fold cross validation and BOW
# plt.figure()
# plt.title("5-fold cross validation + BOW_Models Accuracy Comparison")
# plt.plot([100, 500, 1000, 5000, 8000], [0.633542977, 0.68427673, 0.726065688, 0.783647799, 0.79301188], label="Logisic Regression Model", color = 'purple')
# plt.plot([100, 500, 1000, 5000, 8000], [0.626974144, 0.681761006, 0.717260657, 0.778057303, 0.787002096], label="Naive Bayes")
# plt.plot([100, 500, 1000, 5000, 8000], [0.631027254, 0.668204053, 0.695317959, 0.708315863, 0.709014675], label="Random Forest")
# plt.plot([100, 500, 1000, 5000, 8000], [0.624039133,0.67966457, 0.719357093, 0.767714885,0.776799441], label="SVM")
# plt.xlabel("Number of Features")
# plt.ylabel("Accuracy")
# plt.legend()
# plt.savefig('5-fold cross validation + BOW_Models Accuracy Comparison')
# plt.show()
# #------------------------------------------------------------------------------------------------------------------------------------------------------------------------
# #Accuracy Comparison for all four models with 5 fold cross validation and Tf-idf
# plt.figure()
# plt.title("5-fold cross validation + Tf-idf_Models Accuracy Comparison")
# plt.plot([100, 500, 1000, 5000, 8000], [0.736687631, 0.775960867, 0.802655486, 0.830328442, 0.836477987], label="Logisic Regression Model")
# plt.plot([100, 500, 1000, 5000, 8000], [0.721872816, 0.762264151, 0.782250175,0.827113906, 0.833682739], label="Naive Bayes")
# plt.plot([100, 500, 1000, 5000, 8000], [0.735569532, 0.765059399, 0.777777778,0.789797345, 0.792173305], label="Random Forest")
# plt.plot([100, 500, 1000, 5000, 8000], [0.734730957,0.780852551, 0.801537386,0.827952481,0.833403215], label="SVM")
# plt.xlabel("Number of Features")
# plt.ylabel("Accuracy")
# plt.legend()
# plt.savefig('5-fold cross validation + Tf_idf_Models Accuracy Comparison')
# plt.show()
# #---------------------------------------------------------------------------------------------------------------------------------------------------------------------
# #Mean_Square Error Comparison for all four models with 5 fold cross validation and BOW
# plt.figure()
# plt.title("5-fold cross validation + BOW_Models Mean Square Error Comparison")
# plt.plot([100, 500, 1000, 5000, 8000], [0.605356939,0.561892579, 0.523387344, 0.465136755, 0.454959471], label="Logisic Regression Model")
# plt.plot([100, 500, 1000, 5000, 8000], [0.610758427,0.564126753, 0.531732398, 0.471107947, 0.461516959], label="Naive Bayes")
# plt.plot([100, 500, 1000, 5000, 8000], [0.607431269, 0.576017315, 0.551980109, 0.540077899, 0.539430556], label="Random Forest")
# plt.plot([100, 500, 1000, 5000, 8000], [0.613156478,0.565981828, 0.529757404,0.481959661,0.472441064], label="SVM")
# plt.xlabel("Number of Features")
# plt.ylabel("Mean Squared error")
# plt.legend()
# plt.savefig('5-fold cross validation + BOW_Models Mean Square Error Comparison')
# plt.show()
# #---------------------------------------------------------------------------------------------------------------------------------------------------------------
#
# #Mean Square Error Comparison for all four models with 5 fold cross validation and Tf-idf
# plt.figure()
# plt.title("5-fold cross validation + Tf-idf_Models Mean Square Error Comparison")
# plt.plot([100, 500, 1000, 5000, 8000], [0.513139717,0.473327723, 0.444234751, 0.411912076, 0.404378551], label="Logisic Regression Model")
# plt.plot([100, 500, 1000, 5000, 8000], [0.527377648,0.487581633, 0.466636717, 0.415795735, 0.407820133], label="Naive Bayes")
# plt.plot([100, 500, 1000, 5000, 8000], [0.514228031, 0.484706716, 0.471404521, 0.458478631,0.455880132], label="Random Forest")
# plt.plot([100, 500, 1000, 5000, 8000], [0.515042758,0.468131872, 0.445491429,0.414786113,0.408162695], label="SVM")
# plt.xlabel("Number of Features")
# plt.ylabel("Mean Squared error")
# plt.legend()
# plt.savefig('5-fold cross validation + Tf-idf_Models Mean Square Error Comparison')
# plt.show()
#
#
# #---------------------------------------------------------------------------------------------------------------------------------------------------------------
#
# #F1 Score Comparison for LR
# plt.figure()
# plt.title("F1 score comparison of Logistic Regression  for BOW and Tf-idf")
# plt.plot([100, 500, 1000, 5000, 8000], [0.74, 0.78,0.80,0.83,0.84], label="Logisic Regression Model + Tf-idf")
# plt.plot([100, 500, 1000, 5000, 8000], [0.62, 0.68, 0.72, 0.78, 0.79], label="Logisic Regression Model + BOW")
# plt.xlabel("Number of Features")
# plt.ylabel("F1 Score")
# plt.legend()
# plt.savefig('F1 score comparison of Logistic Regression  for BOW and Tf-idf')
# plt.show()
# #---------------------------------------------------------------------------------------------------------------------------------------------------------------
#
# #F1 Score Comparison for NB
# plt.figure()
# plt.title("F1 score comparison of Naive Bayes for BOW and Tf-idf")
# plt.plot([100, 500, 1000, 5000, 8000], [0.72, 0.76,0.78,0.83,0.83], label="Naive Bayes + Tf-idf", color = 'purple')
# plt.plot([100, 500, 1000, 5000, 8000], [0.61, 0.68, 0.72, 0.78, 0.79], label="Naive Bayes + BOW", color = 'deepskyblue')
# plt.xlabel("Number of Features")
# plt.ylabel("F1 Score")
# plt.legend()
# plt.savefig('F1 Score  Comparison of Naive Bayes Model for BOW and Tf-idf')
# plt.show()
# #---------------------------------------------------------------------------------------------------------------------------------------------------------------
#
# #F1 Score Comparison for RF
# plt.figure()
# plt.title("F1 score comparison of Random Forest  for BOW and Tf-idf")
# plt.plot([100, 500, 1000, 5000, 8000], [0.74, 0.76,0.78,0.79,0.79], label="Random Forest + Tf-idf", color = 'red')
# plt.plot([100, 500, 1000, 5000, 8000], [0.61, 0.65, 0.68, 0.69, 0.70], label="Random Forest + BOW", color = 'darkblue')
# plt.xlabel("Number of Features")
# plt.ylabel("F1 Score")
# plt.legend()
# plt.savefig('F1 score comparison of Random Forest for BOW and Tf-idf')
# plt.show()
# #---------------------------------------------------------------------------------------------------------------------------------------------------------------
#
# #F1 Score Comparison for SVM
#
#
# plt.figure()
# plt.title("F1 score comparison of SVM  for BOW and Tf-idf")
# plt.plot([100, 500, 1000, 5000, 8000], [0.73, 0.78,0.80,0.83,0.83], label="SVM + Tf-idf", color ='teal')
# plt.plot([100, 500, 1000, 5000, 8000], [0.60, 0.67, 0.72, 0.77, 0.78], label="SVM+ BOW", color ='magenta')
# plt.xlabel("Number of Features")
# plt.ylabel("F1 Score")
# plt.legend()
# plt.savefig('F1 score comparison of SVM  for BOW and Tf-idf')
# plt.show()
#
# #---------------------------------------------------------------------------------------------------------------------------------------------------------------
# #Comparing SVM and Naive Bayes:
# plt.figure()
# plt.title("Comapring SVM and Naive Bayes,5-fold cross validation + Tf-idf")
# plt.plot([100, 500, 1000, 5000, 8000], [0.721872816, 0.762264151, 0.782250175,0.827113906, 0.833682739], label="Naive Bayes Accuracy")
# plt.plot([100, 500, 1000, 5000, 8000], [0.734730957,0.780852551, 0.801537386,0.827952481,0.833403215], label="SVM Accuracy")
# plt.plot([100, 500, 1000, 5000, 8000], [0.527377648,0.487581633, 0.466636717, 0.415795735, 0.407820133], label="Naive Bayes Mean Square Error")
# plt.plot([100, 500, 1000, 5000, 8000], [0.515042758,0.468131872, 0.445491429,0.414786113,0.408162695], label="SVM Mean Square Error")
#
# plt.xlabel("Number of Features")
# plt.ylabel("Accuracy an Mean Square error")
# plt.legend()
# plt.savefig('Comapring SVM and Naive Bayes,5-fold cross validation + Tf-idf')
# plt.show()
#
# #Comparing the AUC score:
# plt.figure()
# plt.title("5-fold cross validation + Tf-idf_Models AUC Comparison")
# plt.plot([100, 500, 1000, 5000, 8000], [0.818975,0.864813905,0.885463719,0.911751751,0.915821543], label="Logisic Regression Model")
# plt.plot([100, 500, 1000, 5000, 8000], [0.81304184,0.853980666,0.874391643,0.908640443,0.916013129], label="Naive Bayes")
# plt.plot([100, 500, 1000, 5000, 8000], [0.81882071181932, 0.8545819119728403, 0.8649514246933474, 0.8757580611062641,0.877073272], label="Random Forest")
# plt.plot([100, 500, 1000, 5000, 8000], [0.739610819, 0.78206851,0.801912445,0.827420573,0.833059535], label="SVM")
# plt.xlabel("Number of Features")
# plt.ylabel("AUC Score")
# plt.legend()
# plt.savefig('5-fold cross validation + Tf-idf_Models AUC Comparison')
# plt.show()

#-------------------------------------------------------------------------------------------------------------------------------------------------
# plt.figure()
# plt.title("Plot for mean system delay for the range of offered load values_Single Queue Simulation")
# plt.plot([0.4, 0.5,0.6,0.7,0.8,0.9], [0.0013309684536212194, 0.001600755060831769, 0.0020013471928890573, 0.0026455801986369504, 0.003892196078147225,0.00813443296914336], label="Mean Delay for range of offered loads")
#
#
# plt.xlabel("Offered Load(in Erlangs)")
# plt.ylabel("Mean System Delay")
# plt.legend()
# plt.savefig('Mean System Delay')
# plt.show()

# #---------------------------------------------------------------------------------------------------

plt.figure()
plt.title("Comparison of Mean Delay of Drop Tail & RED")
plt.plot([0.7,0.8,0.9, 1.0, 1.1,1.2,1.3,1.4], [


0.002689962332295467,
0.003956791411498215,
0.007845042585373775,
0.025024456369502058,
 0.07844234528078838,
0.1360621475898541,
 0.18502888606957094,
0.22817975211700575

], label="Mean Delay of Drop Tail")
plt.plot([0.7,0.8,0.9, 1.0, 1.1,1.2,1.3,1.4], [

0.00265635111823687,
0.003977368993644641,
0.007757930987115922,
0.026191787020699676,
 0.05964949533120085,
0.103381146214731,
0.14429456697225377,
0.18226010470248334

], label="Mean Delay of RED")


plt.xlabel("Offered Load(in Erlangs)")
plt.ylabel("Mean System Delay")
plt.legend()
plt.savefig('Comparison of Mean Delay of Drop Tail & RED')
plt.show()

#-------------------------------------------------------------------------------------------------------
#
plt.figure()
plt.title("Comparison of Normalised throughput of Drop Tail & RED")
plt.plot([0.7,0.8,0.9, 1.0, 1.1,1.2,1.3,1.4], [

0.788436193530087,
0.853422484111602,
0.960233190176975,
0.993657589989865,
0.99724086321192735,
0.99994796841041021,
0.99998951545513549,
0.9999998360029914

], label="Normalised Throughput of RED")
plt.plot([0.7,0.8,0.9, 1.0, 1.1,1.2,1.3,1.4], [


0.688171053338884,
0.788013579679079,
0.889090538440732,
0.965818680363365,
0.9933641082268611,
0.999435346412231,
0.99987680349857,
0.9999412015166

], label="Normalised Throughput of Drop Tail")


plt.xlabel("Offered Load(in Erlangs)")
plt.ylabel("Normalised Throughput")
plt.legend()
plt.savefig('Comparison of Normalised throughput of Drop Tail & RED')
plt.show()

#-------------------------------------------------------------------------------------------------

plt.figure()
plt.title("Instantaneous Queue Size Vs Time")
plt.plot([1,
2,
3,
4,
5,
6,
7,
8,
9,
10,
11,
12,
13,
14,
15,
16,
17,
18,
19,
20,
21,
22,
23,
24,
25,
26,
27,
28,
29,
30,
31,
32,
33,
34,
35,
36,
37,
38,
39,
40,
41,
42,
43,
44,
45,
46,
47,
48,
49,
50,
51,
52,
53,
54,
55,
56,
57,
58,
59,
60,
61,
62,
63,
64,
65,
66,
67,
68,
69,
70,
71,
72,
73,
74,
75,
76,
77,
78,
79,
80,
81,
82,
83,
84,
85,
86,
87,
88,
89,
90,
91,
92,
93,
94,
95,
96,
97,
98,
99,
100,
101,
102,
103,
104,
105,
106,
107,
108,
109,
110,
111,
112,
113,
114,
115,
116,
117,
118,
119,
120,
121,
122,
123,
124,
125,
126,
127,
128,
129,
130,
131,
132,
133,
134,
135,
136,
137,
138,
139,
140,
141,
142,
143,
144,
145,
146,
147,
148,
149,
150,
151,
152,
153,
154,
155,
156,
157,
158,
159,
160,
161,
162,
163,
164,
165,
166,
167,
168,
169,
170,
171,
172,
173,
174,
175,
176,
177,
178,
179,
180,
181,
182,
183,
184,
185,
186,
187,
188,
189,
190,
191,
192,
193,
194,
195,
196,
197,
198,
199,
200,
201,
202,
203,
204,
205,
206,
207,
208,
209,
210,
211,
212,
213,
214,
215,
216,
217,
218,
219,
220,
221,
222,
223,
224,
225,
226,
227,
228,
229,
230,
231,
232,
233,
234,
235,
236,
237,
238,
239,
240,
241,
242,
243,
244,
245,
246,
247,
248,
249,
250,
251,
252,
253,
254,
255,
256,
257,
258,
259,
260,
261,
262,
263,
264,
265,
266,
267,
268,
269,
270,
271,
272,
273,
274,
275,
276,
277,
278,
279,
280,
281,
282,
283,
284,
285,
286,
287,
288,
289,
290,
291,
292,
293,
294,
295,
296,
297,
298,
299,
300,
301,
302,
303,
304,
305,
306,
307,
308,
309,
310,
311,
312,
313,
314,
315,
316,
317,
318,
319,
320,
321,
322,
323,
324,
325,
326,
327,
328,
329,
330,
331,
332,
333,
334,
335,
336,
337,
338,
339,
340,
341,
342,
343,
344,
345,
346,
347,
348,
349,
350,
351,
352,
353,
354,
355,
356,
357,
358,
359,
360,
361,
362,
363,
364,
365,
366,
367,
368,
369,
370,
371,
372,
373,
374,
375,
376,
377,
378,
379,
380,
381,
382,
383,
384,
385,
386,
387,
388,
389,
390,
391,
392,
393,
394,
395,
396,
397,
398,
399,
400,
401,
402,
403,
404,
405,
406,
407,
408,
409,
410,
411,
412,
413,
414,
415,
416,
417,
418,
419,
420,
421,
422,
423,
424,
425,
426,
427,
428,
429,
430,
431,
432,
433,
434,
435,
436,
437,
438,
439,
440,
441,
442,
443,
444,
445,
446,
447,
448,
449,
450,
451,
452,
453,
454,
455,
456,
457,
458,
459,
460,
461,
462,
463,
464,
465,
466,
467,
468,
469,
470,
471,
472,
473,
474,
475,
476,
477,
478,
479,
480,
481,
482,
483,
484,
485,
486,
487,
488,
489,
490,
491,
492,
493,
494,
495,
496,
497,
498,
499,
500,
501,
502,
503,
504,
505,
506,
507,
508,
509,
510,
511,
512,
513,
514,
515,
516,
517,
518,
519,
520,
521,
522,
523,
524,
525,
526,
527,
528,
529,
530,
531,
532,
533,
534,
535,
536,
537,
538,
539,
540,
541,
542,
543,
544,
545,
546,
547,
548,
549,
550,
551,
552,
553,
554,
555,
556,
557,
558,
559,
560,
561,
562,
563,
564,
565,
566,
567,
568,
569,
570,
571,
572,
573,
574,
575,
576,
577,
578,
579,
580,
581,
582,
583,
584,
585,
586,
587,
588,
589,
590,
591,
592,
593,
594,
595,
596,
597,
598,
599,
600,
601,
602,
603,
604,
605,
606,
607,
608,
609,
610,
611,
612,
613,
614,
615,
616,
617,
618,
619,
620,
621,
622,
623,
624,
625,
626,
627,
628,
629,
630,
631,
632,
633,
634,
635,
636,
637,
638,
639,
640,
641,
642,
643,
644,
645,
646,
647,
648,
649,
650,
651,
652,
653,
654,
655,
656,
657,
658,
659,
660,
661,
662,
663,
664,
665,
666,
667,
668,
669,
670,
671,
672,
673,
674,
675,
676,
677,
678,
679,
680,
681,
682,
683,
684,
685,
686,
687,
688,
689,
690,
691,
692,
693,
694,
695,
696,
697,
698,
699,
700,
701,
702,
703,
704,
705,
706,
707,
708,
709,
710,
711,
712,
713,
714,
715,
716,
717,
718,
719,
720,
721,
722,
723,
724,
725,
726,
727,
728,
729,
730,
731,
732,
733,
734,
735,
736,
737,
738,
739,
740,
741,
742,
743,
744,
745,
746,
747,
748,
749,
750,
751,
752,
753,
754,
755,
756,
757,
758,
759,
760,
761,
762,
763,
764,
765,
766,
767,
768,
769,
770,
771,
772,
773,
774,
775,
776,
777,
778,
779,
780,
781,
782,
783,
784,
785,
786,
787,
788,
789,
790,
791,
792,
793,
794,
795,
796,
797,
798,
799,
800,
801,
802,
803,
804,
805,
806,
807,
808,
809,
810,
811,
812,
813,
814,
815,
816,
817,
818,
819,
820,
821,
822,
823,
824,
825,
826,
827,
828,
829,
830,
831,
832,
833,
834,
835,
836,
837,
838,
839,
840,
841,
842,
843,
844,
845,
846,
847,
848,
849,
850,
851,
852,
853,
854,
855,
856,
857,
858,
859,
860,
861,
862,
863,
864,
865,
866,
867,
868,
869,
870,
871,
872,
873,
874,
875,
876,
877,
878,
879,
880,
881,
882,
883,
884,
885,
886,
887,
888,
889,
890,
891,
892,
893,
894,
895,
896,
897,
898,
899,
900,
901,
902,
903,
904,
905,
906,
907,
908,
909,
910,
911,
912,
913,
914,
915,
916,
917,
918,
919,
920,
921,
922,
923,
924,
925,
926,
927,
928,
929,
930,
931,
932,
933,
934,
935,
936,
937,
938,
939,
940,
941,
942,
943,
944,
945,
946,
947,
948,
949,
950,
951,
952,
953,
954,
955,
956,
957,
958,
959,
960,
961,
962,
963,
964,
965,
966,
967,
968,
969,
970,
971,
972,
973,
974,
975,
976,
977,
978,
979,
980,
981,
982,
983,
984,
985,
986,
987,
988,
989,
990,
991,
992,
993,
994,
995,
996,
997,
998,
999,
1000,
1001,
1002,
1003,
1004,
1005,
1006,
1007,
1008,
1009,
1010,
1011,
1012,
1013,
1014,
1015,
1016,
1017,
1018,
1019,
1020,
1021,
1022,
1023,
1024,
1025,
1026,
1027,
1028,
1029,
1030,
1031,
1032,
1033,
1034,
1035,
1036,
1037,
1038,
1039,
1040,
1041,
1042,
1043,
1044,
1045,
1046,
1047,
1048,
1049,
1050,
1051,
1052,
1053,
1054,
1055,
1056,
1057,
1058,
1059,
1060,
1061,
1062,
1063,
1064,
1065,
1066,
1067,
1068,
1069,
1070,
1071,
1072,
1073,
1074,
1075,
1076,
1077,
1078,
1079,
1080,
1081,
1082,
1083,
1084,
1085,
1086,
1087,
1088,
1089,
1090,
1091,
1092,
1093,
1094,
1095,
1096,
1097,
1098,
1099,
1100,
1101,
1102,
1103,
1104,
1105,
1106,
1107,
1108,
1109,
1110,
1111,
1112,
1113,
1114,
1115,
1116,
1117,
1118,
1119,
1120,
1121,
1122,
1123,
1124,
1125,
1126,
1127,
1128,
1129,
1130,
1131,
1132,
1133,
1134,
1135,
1136,
1137,
1138,
1139,
1140,
1141,
1142,
1143,
1144,
1145,
1146,
1147,
1148,
1149,
1150,
1151,
1152,
1153,
1154,
1155,
1156,
1157,
1158,
1159,
1160,
1161,
1162,
1163,
1164,
1165,
1166,
1167,
1168,
1169,
1170,
1171,
1172,
1173,
1174,
1175,
1176,
1177,
1178,
1179,
1180,
1181,
1182,
1183,
1184,
1185,
1186,
1187,
1188,
1189,
1190,
1191,
1192,
1193,
1194,
1195,
1196,
1197,
1198,
1199,
1200,
1201,
1202,
1203,
1204,
1205,
1206,
1207,
1208,
1209,
1210,
1211,
1212,
1213,
1214,
1215,
1216,
1217,
1218,
1219,
1220,
1221,
1222,
1223,
1224,
1225,
1226,
1227,
1228,
1229,
1230,
1231,
1232,
1233,
1234,
1235,
1236,
1237,
1238,
1239,
1240,
1241,
1242,
1243,
1244,
1245,
1246,
1247,
1248,
1249,
1250,
],[1,
3,
2,
3,
3,
4,
4,
5,
6,
7,
8,
8,
8,
9,
10,
11,
11,
10,
11,
11,
12,
13,
12,
13,
11,
10,
10,
11,
12,
10,
11,
9,
8,
4,
5,
6,
7,
8,
7,
4,
4,
2,
2,
3,
2,
3,
3,
3,
3,
4,
5,
6,
5,
6,
7,
6,
7,
5,
5,
6,
5,
6,
7,
8,
5,
1,
2,
3,
4,
4,
3,
4,
5,
6,
6,
6,
7,
8,
9,
10,
10,
10,
10,
11,
12,
13,
14,
15,
16,
16,
17,
18,
19,
17,
18,
17,
17,
17,
16,
16,
15,
16,
15,
15,
14,
14,
14,
15,
15,
16,
17,
18,
19,
18,
17,
15,
16,
17,
18,
18,
19,
18,
19,
20,
20,
21,
17,
18,
17,
18,
18,
16,
16,
17,
16,
17,
17,
17,
16,
17,
18,
16,
17,
18,
17,
18,
16,
17,
11,
11,
9,
10,
10,
11,
11,
12,
13,
14,
10,
9,
8,
9,
8,
9,
10,
11,
5,
6,
7,
7,
8,
7,
8,
7,
8,
9,
10,
11,
10,
11,
8,
8,
8,
7,
8,
8,
9,
10,
11,
12,
13,
14,
15,
15,
16,
17,
17,
18,
19,
19,
18,
17,
18,
19,
20,
21,
22,
18,
19,
20,
19,
18,
19,
20,
19,
20,
20,
21,
22,
23,
24,
25,
25,
25,
25,
24,
25,
26,
27,
28,
27,
28,
29,
30,
31,
29,
30,
29,
30,
31,
30,
29,
28,
29,
27,
27,
28,
29,
29,
30,
30,
29,
29,
30,
31,
31,
31,
31,
31,
31,
31,
31,
27,
28,
29,
30,
31,
31,
28,
29,
29,
30,
31,
31,
31,
31,
31,
30,
31,
31,
30,
31,
31,
30,
30,
30,
31,
29,
29,
30,
26,
27,
28,
27,
28,
27,
28,
29,
28,
27,
28,
29,
29,
30,
30,
30,
29,
30,
31,
28,
29,
29,
30,
31,
30,
31,
30,
31,
31,
31,
30,
30,
31,
30,
31,
31,
31,
29,
30,
30,
30,
29,
30,
31,
31,
31,
31,
31,
27,
28,
29,
30,
31,
31,
29,
29,
30,
31,
30,
29,
30,
28,
25,
20,
21,
22,
22,
23,
22,
23,
23,
22,
22,
23,
22,
23,
24,
24,
25,
25,
23,
24,
24,
23,
24,
25,
26,
27,
27,
28,
28,
24,
24,
24,
23,
24,
25,
26,
22,
23,
24,
24,
25,
24,
25,
26,
26,
27,
28,
29,
30,
31,
30,
31,
31,
31,
31,
30,
31,
31,
30,
29,
30,
30,
31,
29,
30,
31,
30,
26,
25,
26,
26,
27,
27,
27,
27,
28,
29,
30,
31,
30,
29,
30,
31,
31,
31,
31,
27,
25,
26,
24,
25,
26,
24,
25,
23,
23,
24,
25,
26,
25,
26,
27,
28,
29,
29,
30,
30,
31,
29,
30,
31,
31,
28,
24,
24,
25,
23,
24,
25,
26,
27,
27,
28,
28,
27,
28,
29,
29,
30,
30,
31,
31,
30,
26,
27,
28,
29,
30,
30,
28,
29,
30,
31,
30,
31,
31,
31,
30,
31,
30,
31,
29,
30,
31,
30,
28,
29,
30,
29,
30,
31,
31,
31,
29,
28,
28,
29,
29,
30,
31,
31,
26,
27,
26,
26,
27,
27,
25,
26,
27,
27,
27,
27,
26,
25,
20,
21,
20,
21,
20,
21,
22,
23,
24,
22,
21,
22,
23,
22,
21,
22,
21,
21,
21,
22,
23,
24,
25,
26,
27,
28,
27,
28,
26,
26,
27,
25,
26,
25,
23,
24,
22,
23,
24,
25,
26,
27,
24,
25,
26,
27,
27,
28,
28,
29,
30,
31,
30,
30,
30,
31,
28,
28,
27,
28,
27,
27,
28,
29,
30,
31,
31,
25,
25,
24,
19,
20,
19,
16,
16,
17,
16,
16,
16,
17,
18,
19,
20,
20,
19,
20,
20,
17,
18,
18,
18,
18,
18,
18,
18,
19,
19,
20,
21,
22,
23,
23,
24,
23,
24,
25,
26,
25,
26,
27,
28,
29,
30,
30,
30,
31,
31,
30,
30,
31,
30,
27,
26,
27,
27,
22,
23,
23,
23,
22,
23,
23,
19,
19,
20,
21,
22,
22,
23,
24,
25,
22,
22,
23,
22,
23,
23,
21,
22,
21,
22,
23,
24,
24,
25,
26,
27,
28,
29,
30,
30,
31,
25,
25,
25,
26,
27,
28,
29,
30,
29,
30,
31,
30,
31,
29,
30,
31,
31,
31,
28,
29,
30,
31,
31,
29,
28,
28,
29,
30,
27,
28,
28,
27,
26,
27,
28,
28,
26,
27,
27,
28,
29,
30,
30,
31,
30,
28,
28,
28,
29,
30,
31,
27,
27,
28,
29,
30,
30,
31,
30,
31,
31,
31,
31,
30,
29,
29,
30,
30,
28,
28,
28,
29,
30,
30,
29,
29,
30,
31,
31,
31,
31,
31,
27,
27,
28,
29,
30,
31,
29,
30,
28,
29,
28,
29,
30,
31,
31,
31,
27,
28,
29,
30,
31,
31,
31,
31,
31,
31,
28,
27,
28,
29,
29,
30,
31,
31,
30,
31,
31,
27,
28,
29,
29,
27,
28,
27,
24,
24,
24,
24,
24,
25,
26,
27,
26,
26,
24,
24,
25,
25,
25,
26,
27,
26,
26,
27,
26,
27,
28,
28,
29,
30,
31,
30,
31,
28,
28,
28,
29,
29,
30,
30,
29,
29,
28,
29,
29,
28,
28,
28,
29,
28,
27,
27,
28,
29,
29,
30,
31,
31,
29,
29,
30,
31,
31,
29,
30,
30,
31,
31,
28,
29,
30,
28,
29,
30,
31,
31,
31,
29,
30,
31,
30,
29,
28,
29,
30,
31,
31,
25,
25,
26,
26,
26,
26,
27,
28,
28,
29,
29,
30,
29,
30,
31,
31,
31,
31,
31,
31,
30,
29,
30,
31,
28,
29,
30,
29,
30,
31,
30,
31,
31,
30,
30,
31,
31,
30,
31,
30,
31,
31,
29,
28,
29,
30,
31,
31,
30,
31,
31,
29,
29,
29,
28,
27,
27,
28,
27,
28,
26,
27,
28,
29,
30,
30,
29,
29,
30,
31,
31,
30,
29,
29,
29,
30,
30,
31,
31,
30,
31,
25,
25,
26,
26,
27,
28,
29,
30,
29,
30,
31,
28,
29,
30,
31,
31,
31,
29,
26,
27,
28,
27,
28,
26,
24,
24,
25,
26,
27,
27,
28,
29,
30,
30,
27,
27,
28,
28,
29,
30,
31,
29,
29,
30,
28,
25,
24,
24,
24,
25,
26,
27,
28,
28,
28,
27,
25,
26,
27,
28,
25,
25,
26,
27,
28,
28,
29,
29,
28,
28,
25,
26,
27,
28,
28,
25,
26,
27,
28,
29,
29,
28,
28,
26,
27,
28,
26,
25,
25,
25,
26,
26,
27,
28,
21,
20,
20,
21,
21,
22,
21,
21,
22,
21,
21,
22,
23,
24,
25,
26,
26,
27,
27,
28,
29,
30,
31,
31,
31,
31,
30,
31,
31,
31,
30,
31,
30,
31,
31,
31,
30,
31,
30,
30,
31,
30,
31,
31,
31,
31,
31,
31,
31,
31,
31,
30,
27,
28,
29,
30,
31,
30,
31,
30,
31,
31,
29,
30,
30,
31,
31,
29,
29,
27,
28,
29,
30,
31,
31,
31,
31,
27,
27,
28,
28,
25,
26,
23,
21,
22,
22,
22,
23,
23,
24,
23,
24,
24,
25,
26,
27,
26,
27,
28,
26,
25,
22,
23,
24,
25,
26,
26,
27,
26,
26,
24,
24,
24,
24,
25,
26,
26,
27,
27,
27,
27,
28,
29,
29,
30,
28,
27,
27,
28,
28,
28,
26,
24,
25,
26,
27,
28,
29,
27,
26,
23,
23,
24,
24,
25,
25,
26,
27,
28,
29,
29,
30,
30,
30,
31,
30,
29,
30,
31,
31,
30,
31,
31,
31,
31,
31,])
plt.xlabel("time")
plt.ylabel("Instantaneous queue size")
plt.legend()
plt.savefig('Instantaneous queue size vs time')
plt.show()




















