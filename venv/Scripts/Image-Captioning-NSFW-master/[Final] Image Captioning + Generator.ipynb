{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"[Final] Image Captioning + Generator.ipynb","provenance":[{"file_id":"18i6vSy4g8FC25K5SNeZMgcgE_VlNQcNW","timestamp":1597750034299}],"collapsed_sections":[],"mount_file_id":"1cy3Vkz8O7jsxcIq82jGSb4fWnzCaEvDA","authorship_tag":"ABX9TyPLa4FqaslAjL63/zzC9CJV"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"533edc140a25473298a418adeb11ab0e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_f7afff7077714cae9cde8f2a9a5ba86b","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_b59588541faa40f5a13526630fa5f470","IPY_MODEL_03e322c5aebc4b6b8c8669383838c687"]}},"f7afff7077714cae9cde8f2a9a5ba86b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b59588541faa40f5a13526630fa5f470":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_365470ddad9a40999dbde7a432c02c25","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":0,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_f5ae5963e1bf4d54a800d6fa9112d006"}},"03e322c5aebc4b6b8c8669383838c687":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_7300c4465f184c499af14c932a5ff9a0","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 0/? [00:00&lt;?, ?it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_d04dfbc84e5e46abba8e054b5ea30437"}},"365470ddad9a40999dbde7a432c02c25":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"f5ae5963e1bf4d54a800d6fa9112d006":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7300c4465f184c499af14c932a5ff9a0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"d04dfbc84e5e46abba8e054b5ea30437":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"90273cd831cc4c0aa0bd19eaec4bbd0c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_52fcfbaaa3da4995912a75d7106d9ea5","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_ffe4383f7dac44efb5333cbab91e99b3","IPY_MODEL_20c3184733ba454d9cf3d8e658af9132"]}},"52fcfbaaa3da4995912a75d7106d9ea5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ffe4383f7dac44efb5333cbab91e99b3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_dae275c5d0914091a5a1a94f8796c942","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":8091,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":8091,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_7780b751019a44b6b46db595bf3e448a"}},"20c3184733ba454d9cf3d8e658af9132":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_172d4e331fac417196accb87d61e68b1","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 8091/8091 [21:10&lt;00:00,  6.37it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_cf95e022849946a399dce72850919fc2"}},"dae275c5d0914091a5a1a94f8796c942":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"7780b751019a44b6b46db595bf3e448a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"172d4e331fac417196accb87d61e68b1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"cf95e022849946a399dce72850919fc2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"id":"SSC8TESlz0vL","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1594232909828,"user_tz":-60,"elapsed":923,"user":{"displayName":"Aayush Jain","photoUrl":"","userId":"02816052941629440884"}},"outputId":"f75c4cff-00a9-43de-bafc-e9ead13a29bd"},"source":["from google.colab import drive\n","drive.mount('/content/drive/')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"3vidzRgXKcLi","colab_type":"code","colab":{}},"source":["#If the program comes across any errors while training the model, please check the keras and tensorflow version installed\n","#Install keras version 2.3.1 and tensorflow version 2.2 for optimal performance"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EoTnvan8E0Q1","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":187},"executionInfo":{"status":"ok","timestamp":1598349863921,"user_tz":-60,"elapsed":7471,"user":{"displayName":"Aayush Jain","photoUrl":"","userId":"02816052941629440884"}},"outputId":"91b60cf7-71c1-4441-f5c9-ae782e4b7522"},"source":["#pip uninstall keras"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Uninstalling Keras-2.4.3:\n","  Would remove:\n","    /usr/local/lib/python3.6/dist-packages/Keras-2.4.3.dist-info/*\n","    /usr/local/lib/python3.6/dist-packages/docs/*\n","    /usr/local/lib/python3.6/dist-packages/keras/*\n","  Would not remove (might be manually added):\n","    /usr/local/lib/python3.6/dist-packages/docs/md_autogen.py\n","    /usr/local/lib/python3.6/dist-packages/docs/update_docs.py\n","Proceed (y/n)? y\n","  Successfully uninstalled Keras-2.4.3\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"mbaVHdbnKvN9","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":360},"executionInfo":{"status":"ok","timestamp":1598349872019,"user_tz":-60,"elapsed":3612,"user":{"displayName":"Aayush Jain","photoUrl":"","userId":"02816052941629440884"}},"outputId":"d40c3cbf-c00b-404f-d072-40c9e98f2108"},"source":["#pip install keras==2.3.1"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting keras==2.3.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/fd/6bfe87920d7f4fd475acd28500a42482b6b84479832bdc0fe9e589a60ceb/Keras-2.3.1-py2.py3-none-any.whl (377kB)\n","\u001b[K     |████████████████████████████████| 378kB 5.5MB/s \n","\u001b[?25hCollecting keras-applications>=1.0.6\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n","\u001b[K     |████████████████████████████████| 51kB 6.6MB/s \n","\u001b[?25hRequirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (1.4.1)\n","Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (1.18.5)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (2.10.0)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (3.13)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (1.15.0)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (1.1.2)\n","Installing collected packages: keras-applications, keras\n","Successfully installed keras-2.3.1 keras-applications-1.0.8\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["keras"]}}},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"ffnLr7sRKxnC","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":238},"executionInfo":{"status":"ok","timestamp":1598349899858,"user_tz":-60,"elapsed":23685,"user":{"displayName":"Aayush Jain","photoUrl":"","userId":"02816052941629440884"}},"outputId":"f19ceaad-fedd-40f5-db73-6e8bb9aa3280"},"source":["#pip uninstall tensorflow"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Uninstalling tensorflow-2.3.0:\n","  Would remove:\n","    /usr/local/bin/estimator_ckpt_converter\n","    /usr/local/bin/saved_model_cli\n","    /usr/local/bin/tensorboard\n","    /usr/local/bin/tf_upgrade_v2\n","    /usr/local/bin/tflite_convert\n","    /usr/local/bin/toco\n","    /usr/local/bin/toco_from_protos\n","    /usr/local/lib/python3.6/dist-packages/tensorflow-2.3.0.dist-info/*\n","    /usr/local/lib/python3.6/dist-packages/tensorflow/*\n","Proceed (y/n)? y\n","  Successfully uninstalled tensorflow-2.3.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8WW4zijrLBTG","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":989},"executionInfo":{"status":"ok","timestamp":1598350010383,"user_tz":-60,"elapsed":97944,"user":{"displayName":"Aayush Jain","photoUrl":"","userId":"02816052941629440884"}},"outputId":"af2cadc6-2bf3-481d-9dbd-93114b897624"},"source":["#pip install tensorflow==2.2"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting tensorflow==2.2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3d/be/679ce5254a8c8d07470efb4a4c00345fae91f766e64f1c2aece8796d7218/tensorflow-2.2.0-cp36-cp36m-manylinux2010_x86_64.whl (516.2MB)\n","\u001b[K     |████████████████████████████████| 516.2MB 32kB/s \n","\u001b[?25hRequirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2) (1.6.3)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2) (1.15.0)\n","Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2) (2.10.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2) (1.1.0)\n","Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2) (1.4.1)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2) (3.3.0)\n","Collecting tensorboard<2.3.0,>=2.2.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1d/74/0a6fcb206dcc72a6da9a62dd81784bfdbff5fedb099982861dc2219014fb/tensorboard-2.2.2-py3-none-any.whl (3.0MB)\n","\u001b[K     |████████████████████████████████| 3.0MB 31kB/s \n","\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2) (1.1.2)\n","Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2) (1.31.0)\n","Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2) (0.2.0)\n","Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2) (0.3.3)\n","Collecting tensorflow-estimator<2.3.0,>=2.2.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a4/f5/926ae53d6a226ec0fda5208e0e581cffed895ccc89e36ba76a8e60895b78/tensorflow_estimator-2.2.0-py2.py3-none-any.whl (454kB)\n","\u001b[K     |████████████████████████████████| 460kB 54.8MB/s \n","\u001b[?25hRequirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2) (3.12.4)\n","Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2) (1.18.5)\n","Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2) (1.12.1)\n","Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2) (0.9.0)\n","Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2) (0.34.2)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (1.7.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (3.2.2)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (1.0.1)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (2.23.0)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (0.4.1)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (49.2.0)\n","Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (1.17.2)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (1.7.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (2020.6.20)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (2.10)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (1.3.0)\n","Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (4.6)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (4.1.1)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (0.2.8)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (3.1.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (3.1.0)\n","Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (0.4.8)\n","Installing collected packages: tensorboard, tensorflow-estimator, tensorflow\n","  Found existing installation: tensorboard 2.3.0\n","    Uninstalling tensorboard-2.3.0:\n","      Successfully uninstalled tensorboard-2.3.0\n","  Found existing installation: tensorflow-estimator 2.3.0\n","    Uninstalling tensorflow-estimator-2.3.0:\n","      Successfully uninstalled tensorflow-estimator-2.3.0\n","Successfully installed tensorboard-2.2.2 tensorflow-2.2.0 tensorflow-estimator-2.2.0\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["tensorboard","tensorflow"]}}},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"ILyBIpbaf_wc","colab_type":"code","colab":{}},"source":["zip_path_train = '/content/drive/My Drive/Python/SUBMISSION FOLDER/Flickr_8k.zip'\n","!cp '{zip_path_train}' .\n","!unzip -q Flickr_8k.zip\n","!rm Flickr_8k.zip"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9IR-N9eKx9Zk","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":137,"referenced_widgets":["533edc140a25473298a418adeb11ab0e","f7afff7077714cae9cde8f2a9a5ba86b","b59588541faa40f5a13526630fa5f470","03e322c5aebc4b6b8c8669383838c687","365470ddad9a40999dbde7a432c02c25","f5ae5963e1bf4d54a800d6fa9112d006","7300c4465f184c499af14c932a5ff9a0","d04dfbc84e5e46abba8e054b5ea30437"]},"executionInfo":{"status":"ok","timestamp":1598350120505,"user_tz":-60,"elapsed":2243,"user":{"displayName":"Aayush Jain","photoUrl":"","userId":"02816052941629440884"}},"outputId":"10bd6243-72dc-4411-cf19-0d2bc83ac1d3"},"source":["import string\n","import numpy as np\n","from PIL import Image\n","import os\n","from pickle import dump, load\n","import numpy as np\n","\n","from keras.applications.xception import Xception, preprocess_input\n","from keras.preprocessing.image import load_img, img_to_array\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.layers.merge import add\n","from keras.models import Model, load_model\n","from keras.layers import Input, Dense, LSTM, Embedding, Dropout\n","\n","from tqdm import tqdm_notebook as tqdm\n","tqdm().pandas()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n","Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"533edc140a25473298a418adeb11ab0e","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"f4xIWxxbyV7a","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1598350124785,"user_tz":-60,"elapsed":1556,"user":{"displayName":"Aayush Jain","photoUrl":"","userId":"02816052941629440884"}},"outputId":"d14c8bd3-151b-4fb3-8378-028e3149e728"},"source":["#Data Preparation\n","#This is the first step for the creation of the Image Captioning model\n","#This process involves cleaning the descriptions and creating a word base for the\n","# model to use while forming sentences.\n","\n","def read_file(filename):\n","    doc = open(filename, 'r')     \n","    text = doc.read()\n","    doc.close()\n","    return text\n","\n","\n","def read_descriptions(filename):\n","    doc = read_file(filename)  \n","    unclean_cap = doc.split('\\n') \n","    descriptions ={}\n","    for cap in unclean_cap[:-1]:\n","        img, cap = cap.split('\\t')\n","        if img[:-2] not in descriptions:\n","            descriptions[img[:-2]] = [ cap ]\n","        else:\n","            descriptions[img[:-2]].append(cap)\n","    return descriptions\n","\n","\n","\n","def process_captions(captions):\n","    table = str.maketrans('','',string.punctuation)\n","    for photo,sent in captions.items():   \n","        for i,pht_desc in enumerate(sent): \n","            pht_desc.replace(\"-\",\" \")\n","            desc = pht_desc.split()\n","            desc = [word.lower() for word in desc]\n","            desc = [word.translate(table) for word in desc]\n","            desc = [word for word in desc if(len(word)>1)]\n","            desc = [word for word in desc if(word.isalpha())]\n","            pht_desc = ' '.join(desc)\n","            captions[photo][i]= pht_desc\n","    return captions\n","\n","def create_vocab(descriptions):\n","    x = set()   \n","    for y in descriptions.keys():   \n","        [x.update(d.split()) for d in descriptions[y]]\n","    return x\n","\n"," \n","def convert_to_txt(descriptions, filename):\n","    l = list()    \n","    for key, desc_list in descriptions.items():\n","        for desc in desc_list:\n","            l.append(key + '\\t' + desc )\n","    words = \"\\n\".join(l)    \n","    doc = open(filename,\"w\") \n","    doc.write(words)\n","    doc.close()\n","\n","\n","flickr8k_text_path = \"/content/Flickr_8k/Flickr8k_Text\"   #Change Path to Flickr8k_text while executing\n","flickr8k_images_path = \"/content/Flickr_8k/Flickr8k_Dataset\"    #Change Path to Flickr8k_dataset while executing\n","document = flickr8k_text_path + \"/\" + \"Flickr8k.token.txt\"  \n","\n","\n","unclean_descriptions = read_descriptions(document)  \n","print(\"The size of descriptions that the project will use for learning is:\",len(unclean_descriptions))\n","clean_descriptions = process_captions(unclean_descriptions)\n","vocabulary = create_vocab(clean_descriptions)\n","print(\"The size of vocabulary that the project will use for learning is:\", len(vocabulary)) \n","convert_to_txt(clean_descriptions, \"descriptions_new.txt\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The size of descriptions that the project will use for learning is: 8092\n","The size of vocabulary that the project will use for learning is: 8763\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9UyWVNKX16Xv","colab_type":"code","colab":{}},"source":["#Creating Xception model instance and preprocessing image before feeding to model\n","\n","def extract_features(directory):\n","        model_xcept = Xception( include_top=False, pooling='avg' )  #model->model_xcept\n","        f = {}    #feature->f\n","        for img in tqdm(os.listdir(directory)):\n","            path = directory + \"/\" + img    #filename->path\n","            photo = Image.open(path)  #image->photo\n","            photo = photo.resize((299,299))\n","            photo = np.expand_dims(photo, axis=0)\n","            photo = photo/127.5\n","            photo = photo - 1.0\n","            feature_extracted = model_xcept.predict(photo)\n","            f[img] = feature_extracted\n","        return f"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AkOD3Qwa1Pke","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":171,"referenced_widgets":["90273cd831cc4c0aa0bd19eaec4bbd0c","52fcfbaaa3da4995912a75d7106d9ea5","ffe4383f7dac44efb5333cbab91e99b3","20c3184733ba454d9cf3d8e658af9132","dae275c5d0914091a5a1a94f8796c942","7780b751019a44b6b46db595bf3e448a","172d4e331fac417196accb87d61e68b1","cf95e022849946a399dce72850919fc2"]},"executionInfo":{"status":"ok","timestamp":1597766411902,"user_tz":-60,"elapsed":745490,"user":{"displayName":"Aayush Jain","photoUrl":"","userId":"02816052941629440884"}},"outputId":"16753c89-bb57-4ef7-9787-34321debb4b7"},"source":["#Creating feature vector and saving it as pickle file for future use\n","\n","xcept_features = extract_features(flickr8k_images_path) \n","dump(xcept_features, open(\"features_new.p\",\"wb\")) "],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/xception/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\n","83689472/83683744 [==============================] - 1s 0us/step\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n","Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n","  import sys\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"90273cd831cc4c0aa0bd19eaec4bbd0c","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=8091.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Q8mqM_we-4Rx","colab_type":"code","colab":{}},"source":["#Uncomment the below code if needed to lead the pickle file manually\n","\n","#features=load(open(\"/content/drive/My Drive/Python/SUBMISSION FOLDER/features.p\",\"rb\"))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0QNpJdFt27u8","colab_type":"code","colab":{}},"source":["#Reading every image and adding START and END token for RNN model to understand captions better\n","\n","\n","def read_image(filename):\n","    doc = read_file(filename)  \n","    images = doc.split(\"\\n\")[:-1] \n","    return images\n","\n","def read_clean_descriptions(filename, photos): \n","    doc = read_file(filename)   \n","    descriptions = {}\n","    for l in doc.split(\"\\n\"):   \n","        words = l.split()\n","        if len(words)<1 :\n","            continue\n","        image, image_caption = words[0], words[1:]\n","        if image in photos:\n","            if image not in descriptions:\n","                descriptions[image] = []\n","            desc = '<start> ' + \" \".join(image_caption) + ' <end>'\n","            descriptions[image].append(desc)\n","    return descriptions\n","\n","def read_features(photos):\n","    complete = load(open(\"features.p\",\"rb\")) \n","    x = {k:complete[k] for k in photos} \n","    return x\n","\n","filename = flickr8k_text_path + \"/\" + \"Flickr_8k.trainImages.txt\"   \n","train_imgs = read_image(filename)\n","train_descriptions = read_clean_descriptions(\"descriptions_new.txt\", train_imgs)\n","train_features = read_features(train_imgs)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nugQCwu9G0VJ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1598350148009,"user_tz":-60,"elapsed":753,"user":{"displayName":"Aayush Jain","photoUrl":"","userId":"02816052941629440884"}},"outputId":"8bc64ed0-a125-4319-e1e5-271863453a3e"},"source":["#Tokeninsing each description (one-hot encoding) and saving them into pickle file\n","\n","\n","def convert_desc(descriptions):\n","    x = []   #all_desc->x\n","    for y in descriptions.keys():   \n","        [x.append(d) for d in descriptions[y]]\n","    return x\n","\n","from keras.preprocessing.text import Tokenizer\n","def word_to_token(descriptions):\n","    desc_list = convert_desc(descriptions)\n","    tokenizer = Tokenizer()\n","    tokenizer.fit_on_texts(desc_list)\n","    return tokenizer\n","\n","tokenizer = word_to_token(train_descriptions)\n","dump(tokenizer, open('tokenizer.p', 'wb'))\n","vocab_size = len(tokenizer.word_index) + 1\n","vocab_size"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["7577"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"Sk1opm7oHVUH","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1598350150307,"user_tz":-60,"elapsed":567,"user":{"displayName":"Aayush Jain","photoUrl":"","userId":"02816052941629440884"}},"outputId":"61e2754c-4a82-41a6-daf6-afb792165e7c"},"source":["#calculating maximum length of descriptions\n","\n","\n","def calc_size(descriptions):\n","    x = convert_desc(descriptions)  \n","    return max(len(d.split()) for d in x)\n","    \n","max_length = calc_size(unclean_descriptions)\n","max_length"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["32"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"vtMrfjwxHfRP","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1598350153049,"user_tz":-60,"elapsed":531,"user":{"displayName":"Aayush Jain","photoUrl":"","userId":"02816052941629440884"}},"outputId":"a1c491ff-2b8f-4d5c-b875-eafeb660040d"},"source":["#create input-output sequence using image generator to provide model with multiple inputs batch wise\n","\n","\n","def data_generator(descriptions, features, tokenizer, max_length):\n","    while 1:\n","        for key, description_list in descriptions.items():\n","            feature = features[key][0]\n","            input_image, input_sequence, output_word = create_sequences(tokenizer, max_length, description_list, feature)\n","            yield [[input_image, input_sequence], output_word]\n","\n","def create_sequences(tokenizer, max_length, desc_list, feature):\n","    X1, X2, y = list(), list(), list()\n","    for desc in desc_list:\n","        seq = tokenizer.texts_to_sequences([desc])[0]\n","        for i in range(1, len(seq)):\n","            in_seq, out_seq = seq[:i], seq[i]\n","            in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n","            out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n","            X1.append(feature)\n","            X2.append(in_seq)\n","            y.append(out_seq)\n","    return np.array(X1), np.array(X2), np.array(y)\n","\n","[a,b],c = next(data_generator(train_descriptions, features, tokenizer, max_length))\n","a.shape, b.shape, c.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((47, 2048), (47, 32), (47, 7577))"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"JU7NAS-JHlrE","colab_type":"code","colab":{}},"source":["#Creating Model structure for merged model. The model makes use of a CNN and LSTM(RNN) for reading features\n","#and generating captions\n","\n","\n","from keras.utils import plot_model\n","def create_CNN_RNN_model(vocab_size, max_length):\n","    ip1 = Input(shape=(2048,))  \n","    xcep_layer1 = Dropout(0.5)(ip1) \n","    xcep_layer2 = Dense(256, activation='relu')(xcep_layer1)  \n","    \n","    ip2 = Input(shape=(max_length,))  \n","    rnn1 = Embedding(vocab_size, 256, mask_zero=True)(ip2)  \n","    rnn2 = Dropout(0.5)(rnn1) \n","    rnn3 = LSTM(256)(rnn2)  \n","    \n","    op1 = add([xcep_layer2, rnn3])  \n","    op2 = Dense(256, activation='relu')(op1)\n","    outputs = Dense(vocab_size, activation='softmax')(op2)\n","    \n","    model_combine = Model(inputs=[ip1, ip2], outputs=outputs)\n","    model_combine.compile(loss='categorical_crossentropy', optimizer='adam')\n","    \n","    print(model_combine.summary())\n","    plot_model(model_combine, to_file='model.png', show_shapes=True)\n","    return model_combine"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"P6uk6Wp_IF3D","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":612},"executionInfo":{"status":"ok","timestamp":1598350159150,"user_tz":-60,"elapsed":1859,"user":{"displayName":"Aayush Jain","photoUrl":"","userId":"02816052941629440884"}},"outputId":"2eb286ae-3d4e-4826-db0f-cac156cdcb23"},"source":["#Final display of all variables to make sure everything is good \n","\n","\n","print('Size of the dataset used for training: ', len(train_imgs))\n","print('Number of clean description for RNN:', len(train_descriptions))\n","print('Number of images for CNN to extract features:', len(train_features))\n","print('Size of vocabulary for RNN to form captions:', vocab_size)\n","print('Length of longest description:', max_length)\n","model_final = create_CNN_RNN_model(vocab_size, max_length)\n","epochs = 10\n","steps = len(train_descriptions)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Size of the dataset used for training:  6000\n","Number of clean description for RNN: 6000\n","Number of images for CNN to extract features: 6000\n","Size of vocabulary for RNN to form captions: 7577\n","Length of longest description: 32\n","Model: \"model_1\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_2 (InputLayer)            (None, 32)           0                                            \n","__________________________________________________________________________________________________\n","input_1 (InputLayer)            (None, 2048)         0                                            \n","__________________________________________________________________________________________________\n","embedding_1 (Embedding)         (None, 32, 256)      1939712     input_2[0][0]                    \n","__________________________________________________________________________________________________\n","dropout_1 (Dropout)             (None, 2048)         0           input_1[0][0]                    \n","__________________________________________________________________________________________________\n","dropout_2 (Dropout)             (None, 32, 256)      0           embedding_1[0][0]                \n","__________________________________________________________________________________________________\n","dense_1 (Dense)                 (None, 256)          524544      dropout_1[0][0]                  \n","__________________________________________________________________________________________________\n","lstm_1 (LSTM)                   (None, 256)          525312      dropout_2[0][0]                  \n","__________________________________________________________________________________________________\n","add_1 (Add)                     (None, 256)          0           dense_1[0][0]                    \n","                                                                 lstm_1[0][0]                     \n","__________________________________________________________________________________________________\n","dense_2 (Dense)                 (None, 256)          65792       add_1[0][0]                      \n","__________________________________________________________________________________________________\n","dense_3 (Dense)                 (None, 7577)         1947289     dense_2[0][0]                    \n","==================================================================================================\n","Total params: 5,002,649\n","Trainable params: 5,002,649\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","None\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"CfXOKUYgIM5m","colab_type":"code","colab":{}},"source":["#Training the merged model for multiple epoch on training data and saving each model with reduced loss \n","\n","\n","os.mkdir(\"models\")\n","for i in range(epochs):\n","    generator = data_generator(train_descriptions, train_features, tokenizer, max_length)\n","    model_final.fit_generator(generator, epochs=1, steps_per_epoch= steps, verbose=1)\n","    model_final.save(\"models/model_\" + str(i) + \".h5\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"N47-Twt_Wyyu","colab_type":"code","colab":{}},"source":["#print captions for 30 random images to demonstrate model capability \n","import matplotlib.pyplot as plt\n","from matplotlib.pyplot import show\n","\n","def preprocess_extract(filename, model):\n","        try:\n","            photo = Image.open(filename)    \n","        except:\n","            print(\"Trouble opening image file, please ensure the path provided is correct\")\n","        photo = photo.resize((299,299))\n","        photo = np.array(photo)\n","        if photo.shape[2] == 4: \n","            photo = photo[..., :3]\n","        photo = np.expand_dims(photo, axis=0)\n","        photo = photo/127.5\n","        photo = photo - 1.0\n","        feature = model.predict(photo)\n","        return feature\n","\n","\n","def tokenize_words(integer, tokenizer):   \n","  for word, index in tokenizer.word_index.items():\n","      if index == integer:\n","          return word\n","  return None\n","\n","  \n","def caption_creator(model, tokenizer, photo, max_length):   \n","    in_text = 'start'\n","    for i in range(max_length):\n","        sequence = tokenizer.texts_to_sequences([in_text])[0]\n","        sequence = pad_sequences([sequence], maxlen=max_length)\n","        pred = model.predict([photo,sequence], verbose=0)\n","        pred = np.argmax(pred)\n","        word = tokenize_words(pred, tokenizer)    \n","        if word is None:\n","            break\n","        in_text += ' ' + word\n","        if word == 'end':\n","            break\n","    return in_text\n","\n","names=[]\n","DATADIR = '/content/Flickr_8k/Flickr8k_Dataset'\n","for img in os.listdir(DATADIR):\n","    names.append(os.path.basename(img))\n","\n","counter=0\n","for i in names:\n","\n","  path = '/content/Flickr_8k/Flickr8k_Dataset/'+os.path.basename(i)  \n","  max_length = 32\n","  tokenizer = load(open(\"tokenizer.p\",\"rb\"))\n","  model = load_model('model_9.h5')    #Include Model path here\n","  model_xcept = Xception(include_top=False, pooling=\"avg\")  \n","  photo = preprocess_extract(path, model_xcept)  \n","  image = Image.open(path)  #caption\n","  caption = caption_creator(model, tokenizer, photo, max_length)  \n","  print(\"\\n\\n\")\n","  print(caption)  \n","  plt.imshow(image) \n","  show()\n","  counter+=1\n","  if counter>=30:\n","    break"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DfMDcNNNW-UH","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}